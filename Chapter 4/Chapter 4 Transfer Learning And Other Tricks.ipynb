{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2af031-9da6-4b99-ac8e-fd07fc4275a6",
   "metadata": {},
   "source": [
    "# Chapter 4: Transfer Learning And Other Tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe68448-55a6-4a68-896a-310e8603ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19fec7ba-5100-4288-ace8-4cf4d6a73d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Python Enviorments\\pytorch_venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Software\\Python Enviorments\\pytorch_venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "transfer_model = models.resnet50(pretrained=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39ff7b-7289-401c-8002-9dbc5d844de7",
   "metadata": {},
   "source": [
    "## Freezing parameters\n",
    "\n",
    "Batch normalization (often labeled as \"bn\" in parameters) normalizes input in neural networks. When you freeze network parameters, they don't update during training. Batch normalization layers should usually remain unfrozen to adapt to data statistics. Excluding parameters with \"bn\" in their names keeps these layers trainable, vital for model performance and convergence, while other layers like convolutional and fully connected ones can be frozen for fine-tuning.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa44150-49e7-422e-8348-b393b31e6c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all model parameters except for batch normalization layers (bn)\n",
    "for name, param in transfer_model.named_parameters():\n",
    "    if(\"bn\" not in name): # \n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5355d2-ae7b-4299-acf8-d8f0667488a3",
   "metadata": {},
   "source": [
    "## Replacing the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5e74d81-33d8-4be0-bc3a-fd25845cfad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.fc = nn.Sequential(nn.Linear(transfer_model.fc.in_features,500),\n",
    "nn.ReLU(),                                 \n",
    "nn.Dropout(), nn.Linear(500,2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73dc4f79-61b2-47d0-89ce-f94f6b276bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output,targets) \n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50a93ab0-bedf-45e4-bcab-d418bb126c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((64,64)),    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225] )\n",
    "    ])\n",
    "train_data_path = \"../Chapter 2/train/\"\n",
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path,transform=img_transforms, is_valid_file=check_image)\n",
    "val_data_path = \"../Chapter 2/val/\"\n",
    "val_data = torchvision.datasets.ImageFolder(root=val_data_path,transform=img_transforms, is_valid_file=check_image)\n",
    "batch_size=64\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader  = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") \n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3caa2b81-ec56-4d65-b24f-b0d5d885dd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "print(len(val_data_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ef908a3-1a1e-4910-ac99-1464cc691747",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.to(device)\n",
    "optimizer = optim.Adam(transfer_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e171af7a-d059-40ea-a4f9-944b850e0aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rsani\\AppData\\Local\\Temp\\ipykernel_12072\\1650489001.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.57, Validation Loss: 0.29, accuracy = 0.88\n",
      "Epoch: 2, Training Loss: 0.19, Validation Loss: 0.46, accuracy = 0.85\n",
      "Epoch: 3, Training Loss: 0.10, Validation Loss: 0.43, accuracy = 0.85\n",
      "Epoch: 4, Training Loss: 0.14, Validation Loss: 0.21, accuracy = 0.94\n",
      "Epoch: 5, Training Loss: 0.03, Validation Loss: 0.23, accuracy = 0.93\n"
     ]
    }
   ],
   "source": [
    "train(transfer_model, optimizer,torch.nn.CrossEntropyLoss(), train_data_loader, val_data_loader, epochs=5,\n",
    "      device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a90c0-1793-46fd-9ebe-38ec00aa5f39",
   "metadata": {},
   "source": [
    "## LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "570e57aa-71f8-4dd5-8cb6-03a42f72d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lr(model, loss_fn, optimizer, train_loader, init_value=1e-8, final_value=10.0, device=\"cpu\"):\n",
    "    # Calculate the number of iterations in an epoch\n",
    "    number_in_epoch = len(train_loader) - 1\n",
    "    # Calculate the update step for learning rate\n",
    "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
    "    # Initialize the learning rate\n",
    "    lr = init_value\n",
    "    # Set the initial learning rate in the optimizer\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "    # Initialize variables to track the best loss, current batch number, and lists for losses and learning rates\n",
    "    best_loss = 0.0\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "\n",
    "    # Loop through the data in the training loader\n",
    "    for data in train_loader:\n",
    "        batch_num += 1\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Check for an exploding loss and return if necessary\n",
    "        if batch_num > 1 and loss > 4 * best_loss:\n",
    "            if len(log_lrs) > 20:\n",
    "                return log_lrs[10:-5], losses[10:-5]\n",
    "            else:\n",
    "                return log_lrs, losses\n",
    "\n",
    "        # Record the best loss\n",
    "        if loss < best_loss or batch_num == 1:\n",
    "            best_loss = loss\n",
    "\n",
    "        # Store the current loss and learning rate values\n",
    "        losses.append(loss.item())\n",
    "        log_lrs.append(lr)\n",
    "\n",
    "        # Perform the backward pass and optimize the model\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate for the next step\n",
    "        lr *= update_step\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "\n",
    "    # Return the learning rates and losses\n",
    "    if len(log_lrs) > 20:\n",
    "        return log_lrs[10:-5], losses[10:-5]\n",
    "    else:\n",
    "        return log_lrs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bba4d773-1b8a-4aaa-a514-0b937fae4bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(lrs, losses) = find_lr(transfer_model, torch.nn.CrossEntropyLoss(),optimizer, train_data_loader,device=device)\n",
    "plt.plot(lrs, losses)\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c52af-8af4-4a33-93da-6a3f519bbb95",
   "metadata": {},
   "source": [
    "## Custom Transforms\n",
    "Here we'll create a lambda transform and a custom transform class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4df25f3-8a07-404d-a34f-0faad3b190ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _random_colour_space(x):\n",
    "    output = x.convert(\"HSV\")\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be2ca697-6aa4-467f-a630-740b7d1f540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "colour_transform = transforms.Lambda(lambda x: _random_colour_space(x))\n",
    "random_colour_transform = torchvision.transforms.RandomApply([colour_transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69b0ecab-3af3-4d3f-8022-18e5e0c10ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise():\n",
    "    \"\"\"Adds gaussian noise to a tensor.\n",
    "    \n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>>     Noise(0.1, 0.05)),\n",
    "        >>> ])\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, mean, stddev):\n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.zeros_like(tensor).normal_(self.mean, self.stddev)\n",
    "        return tensor.add_(noise)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        repr = f\"{self.__class__.__name__  }(mean={self.mean},sttdev={self.stddev})\"\n",
    "        return repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc077255-8427-4209-b3d8-4344073f73db",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transform_pipeline = transforms.Compose([random_colour_transform, Noise(0.1, 0.05)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0211b0f-5a0b-48d6-a3cf-1c7cf0238f84",
   "metadata": {},
   "source": [
    "## Ensembles\n",
    "Given a list of models, we can produce predictions for each model and then make an average to make a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f94fcd7e-4fd2-4412-9d08-e3398dbd5531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rsani\\AppData\\Local\\Temp\\ipykernel_12072\\210517164.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = [F.softmax(m(torch.rand(1,3,224,244).to(device))) for m in models_ensemble]\n"
     ]
    }
   ],
   "source": [
    "models_ensemble = [models.resnet50().to(device), models.resnet50().to(device)]\n",
    "predictions = [F.softmax(m(torch.rand(1,3,224,244).to(device))) for m in models_ensemble] \n",
    "avg_prediction = torch.stack(predictions).mean(0).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57a341be-7838-4e6b-9787-3f295d55742b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23, device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "deb2e13f-e6f3-43c3-b556-4b434c38597c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0056, 0.0011, 0.0012,  ..., 0.0007, 0.0015, 0.0006]],\n",
       "\n",
       "        [[0.0010, 0.0009, 0.0012,  ..., 0.0004, 0.0008, 0.0007]]],\n",
       "       device='cuda:0', grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0a1f71-f1c2-4e1c-8947-1c53f8f0b37e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
